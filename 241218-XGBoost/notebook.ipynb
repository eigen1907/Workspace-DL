{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def init_model_and_scaler(data_dir, detid_table_path, model_dir, n_sample_files=10):\n",
    "    track_paths = sorted(glob.glob(os.path.join(data_dir, '*.csv')))\n",
    "    sample_paths = track_paths[:n_sample_files]\n",
    "\n",
    "    df_list = [pd.read_csv(p) for p in sample_paths]\n",
    "    df_sample = pd.concat(df_list, ignore_index=True)\n",
    "    df_sample['det_raw_id'] = df_sample['det_raw_id'].apply(lambda x: list(map(int, x.split(' ')[:-1])))\n",
    "\n",
    "    detid_table_df = pd.read_csv(detid_table_path)\n",
    "    detid_table = np.sort(detid_table_df['det_raw_id'].unique())\n",
    "\n",
    "    detid_encoder = MultiLabelBinarizer(classes=detid_table)\n",
    "    detid_encoder.fit(df_sample['det_raw_id'])\n",
    "\n",
    "    track_scaler = StandardScaler()\n",
    "    track_scaler.fit(df_sample[['track_pt', 'track_eta', 'track_phi']].values)\n",
    "\n",
    "    X_sample = track_scaler.transform(df_sample[['track_pt', 'track_eta', 'track_phi']].values)\n",
    "    Y_sample = detid_encoder.transform(df_sample['det_raw_id'])\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_sample, Y_sample, test_size=0.1, random_state=42)\n",
    "\n",
    "    model = XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42)\n",
    "    model.fit(X_train, Y_train, eval_set=[(X_val, Y_val)], verbose=True)\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model.save_model(os.path.join(model_dir, 'model'))\n",
    "    joblib.dump(track_scaler, os.path.join(model_dir, 'scaler.pkl'))\n",
    "    joblib.dump(detid_encoder, os.path.join(model_dir, 'encoder.pkl'))\n",
    "\n",
    "def load_data(file_paths, track_scaler, detid_encoder):\n",
    "    df_list = [pd.read_csv(file_path) for file_path in file_paths]\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    df['det_raw_id'] = df['det_raw_id'].apply(lambda x: list(map(int, x.split(' ')[:-1])))\n",
    "\n",
    "    X = track_scaler.transform(df[['track_pt', 'track_eta', 'track_phi']].values)\n",
    "    Y = detid_encoder.transform(df['det_raw_id'])\n",
    "    return X, Y\n",
    "\n",
    "def data_generator(file_paths, batch_size, track_scaler, detid_encoder):\n",
    "    for i in range(0, len(file_paths), batch_size):\n",
    "        batch_files = file_paths[i:i+batch_size]\n",
    "        X_batch, Y_batch = load_data(batch_files, track_scaler, detid_encoder)\n",
    "        yield X_batch, Y_batch\n",
    "\n",
    "def train_model_and_scaler(data_dir, init_model_path, scaler_path, encoder_path, model_dir, n_file=100, batch_size=10):\n",
    "    model = XGBClassifier()\n",
    "    model.load_model(init_model_path)\n",
    "\n",
    "    track_scaler = joblib.load(scaler_path)\n",
    "    detid_encoder = joblib.load(encoder_path)\n",
    "\n",
    "    track_paths = sorted(glob.glob(os.path.join(data_dir, '*.csv')))\n",
    "    train_track_paths = track_paths[:n_file]\n",
    "\n",
    "    for X_batch, Y_batch in data_generator(train_track_paths, batch_size, track_scaler, detid_encoder):\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(X_batch, Y_batch, test_size=0.1, random_state=42)\n",
    "        model.fit(X_train, Y_train, xgb_model=model.get_booster(), eval_set=[(X_val, Y_val)], verbose=True)\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model.save_model(os.path.join(model_dir, 'XGBoost_FinalModel.json'))\n",
    "    joblib.dump(track_scaler, os.path.join(model_dir, 'scaler.pkl'))\n",
    "    joblib.dump(detid_encoder, os.path.join(model_dir, 'encoder.pkl'))\n",
    "\n",
    "def evaluate_model_and_scaler(data_dir, model_path, scaler_path, encoder_path, n_eval_files=10):\n",
    "    model = XGBClassifier()\n",
    "    model.load_model(model_path)\n",
    "\n",
    "    track_scaler = joblib.load(scaler_path)\n",
    "    detid_encoder = joblib.load(encoder_path)\n",
    "\n",
    "    track_paths = sorted(glob.glob(os.path.join(data_dir, '*.csv')))\n",
    "    eval_paths = track_paths[-n_eval_files:]\n",
    "    \n",
    "    X_eval, Y_eval = load_data(eval_paths, track_scaler, detid_encoder)\n",
    "    preds = model.predict(X_eval)\n",
    "    accuracy = (preds == Y_eval).mean()\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.11579\n",
      "[1]\tvalidation_0-logloss:0.10428\n",
      "[2]\tvalidation_0-logloss:0.09400\n",
      "[3]\tvalidation_0-logloss:0.08481\n",
      "[4]\tvalidation_0-logloss:0.07658\n",
      "[5]\tvalidation_0-logloss:0.06919\n",
      "[6]\tvalidation_0-logloss:0.06256\n",
      "[7]\tvalidation_0-logloss:0.05661\n",
      "[8]\tvalidation_0-logloss:0.05125\n",
      "[9]\tvalidation_0-logloss:0.04643\n",
      "[10]\tvalidation_0-logloss:0.04208\n",
      "[11]\tvalidation_0-logloss:0.03817\n",
      "[12]\tvalidation_0-logloss:0.03464\n",
      "[13]\tvalidation_0-logloss:0.03145\n",
      "[14]\tvalidation_0-logloss:0.02858\n",
      "[15]\tvalidation_0-logloss:0.02599\n",
      "[16]\tvalidation_0-logloss:0.02364\n",
      "[17]\tvalidation_0-logloss:0.02152\n",
      "[18]\tvalidation_0-logloss:0.01961\n",
      "[19]\tvalidation_0-logloss:0.01788\n",
      "[20]\tvalidation_0-logloss:0.01631\n",
      "[21]\tvalidation_0-logloss:0.01489\n",
      "[22]\tvalidation_0-logloss:0.01361\n",
      "[23]\tvalidation_0-logloss:0.01245\n",
      "[24]\tvalidation_0-logloss:0.01140\n",
      "[25]\tvalidation_0-logloss:0.01045\n",
      "[26]\tvalidation_0-logloss:0.00959\n",
      "[27]\tvalidation_0-logloss:0.00881\n",
      "[28]\tvalidation_0-logloss:0.00811\n",
      "[29]\tvalidation_0-logloss:0.00747\n",
      "[30]\tvalidation_0-logloss:0.00689\n",
      "[31]\tvalidation_0-logloss:0.00636\n",
      "[32]\tvalidation_0-logloss:0.00589\n",
      "[33]\tvalidation_0-logloss:0.00545\n",
      "[34]\tvalidation_0-logloss:0.00506\n",
      "[35]\tvalidation_0-logloss:0.00471\n",
      "[36]\tvalidation_0-logloss:0.00438\n",
      "[37]\tvalidation_0-logloss:0.00409\n",
      "[38]\tvalidation_0-logloss:0.00383\n",
      "[39]\tvalidation_0-logloss:0.00358\n",
      "[40]\tvalidation_0-logloss:0.00337\n",
      "[41]\tvalidation_0-logloss:0.00317\n",
      "[42]\tvalidation_0-logloss:0.00299\n",
      "[43]\tvalidation_0-logloss:0.00282\n",
      "[44]\tvalidation_0-logloss:0.00268\n",
      "[45]\tvalidation_0-logloss:0.00254\n",
      "[46]\tvalidation_0-logloss:0.00242\n",
      "[47]\tvalidation_0-logloss:0.00231\n",
      "[48]\tvalidation_0-logloss:0.00221\n",
      "[49]\tvalidation_0-logloss:0.00211\n",
      "[50]\tvalidation_0-logloss:0.00203\n",
      "[51]\tvalidation_0-logloss:0.00195\n",
      "[52]\tvalidation_0-logloss:0.00188\n",
      "[53]\tvalidation_0-logloss:0.00182\n",
      "[54]\tvalidation_0-logloss:0.00176\n",
      "[55]\tvalidation_0-logloss:0.00171\n",
      "[56]\tvalidation_0-logloss:0.00166\n",
      "[57]\tvalidation_0-logloss:0.00162\n",
      "[58]\tvalidation_0-logloss:0.00158\n",
      "[59]\tvalidation_0-logloss:0.00154\n",
      "[60]\tvalidation_0-logloss:0.00151\n",
      "[61]\tvalidation_0-logloss:0.00148\n",
      "[62]\tvalidation_0-logloss:0.00145\n",
      "[63]\tvalidation_0-logloss:0.00143\n",
      "[64]\tvalidation_0-logloss:0.00140\n",
      "[65]\tvalidation_0-logloss:0.00138\n",
      "[66]\tvalidation_0-logloss:0.00136\n",
      "[67]\tvalidation_0-logloss:0.00135\n",
      "[68]\tvalidation_0-logloss:0.00133\n",
      "[69]\tvalidation_0-logloss:0.00131\n",
      "[70]\tvalidation_0-logloss:0.00130\n",
      "[71]\tvalidation_0-logloss:0.00129\n",
      "[72]\tvalidation_0-logloss:0.00128\n",
      "[73]\tvalidation_0-logloss:0.00127\n",
      "[74]\tvalidation_0-logloss:0.00126\n",
      "[75]\tvalidation_0-logloss:0.00126\n",
      "[76]\tvalidation_0-logloss:0.00126\n",
      "[77]\tvalidation_0-logloss:0.00126\n",
      "[78]\tvalidation_0-logloss:0.00126\n",
      "[79]\tvalidation_0-logloss:0.00125\n",
      "[80]\tvalidation_0-logloss:0.00125\n",
      "[81]\tvalidation_0-logloss:0.00125\n",
      "[82]\tvalidation_0-logloss:0.00125\n",
      "[83]\tvalidation_0-logloss:0.00125\n",
      "[84]\tvalidation_0-logloss:0.00125\n",
      "[85]\tvalidation_0-logloss:0.00125\n",
      "[86]\tvalidation_0-logloss:0.00125\n",
      "[87]\tvalidation_0-logloss:0.00125\n",
      "[88]\tvalidation_0-logloss:0.00125\n",
      "[89]\tvalidation_0-logloss:0.00125\n",
      "[90]\tvalidation_0-logloss:0.00125\n",
      "[91]\tvalidation_0-logloss:0.00125\n",
      "[92]\tvalidation_0-logloss:0.00125\n",
      "[93]\tvalidation_0-logloss:0.00125\n",
      "[94]\tvalidation_0-logloss:0.00125\n",
      "[95]\tvalidation_0-logloss:0.00125\n",
      "[96]\tvalidation_0-logloss:0.00125\n",
      "[97]\tvalidation_0-logloss:0.00125\n",
      "[98]\tvalidation_0-logloss:0.00125\n",
      "[99]\tvalidation_0-logloss:0.00125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/hep/eigen1907/micromamba/envs/torch-cpu/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [02:08:36] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1733179675237/work/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data_dir = '/users/hep/eigen1907/Workspace/Workspace-DL/241215-track_det_raw_id/TrackDetMatches'\n",
    "    detid_table_path = '/users/hep/eigen1907/Workspace/Workspace-DL/241215-track_det_raw_id/muon_system_det_raw_id.csv'\n",
    "    model_dir = '/users/hep/eigen1907/Workspace/Workspace-DL/241218-XGBoost/model'\n",
    "    init_model_and_scaler(data_dir, detid_table_path, model_dir, n_sample_files=3)\n",
    "    #init_model_path = os.path.join(model_dir, 'model.json')\n",
    "    #scaler_path = os.path.join(model_dir, 'scaler.pkl')\n",
    "    #encoder_path = os.path.join(model_dir, 'encoder.pkl')\n",
    "    #train_model_and_scaler(data_dir, init_model_path, scaler_path, encoder_path, model_dir, n_file=100, batch_size=10)\n",
    "    #final_model_path = os.path.join(model_dir, 'XGBoost_FinalModel.json')\n",
    "    #eval_accuracy = evaluate_model_and_scaler(data_dir, final_model_path, scaler_path, encoder_path, n_eval_files=10)\n",
    "    #print(\"Evaluation accuracy:\", eval_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
